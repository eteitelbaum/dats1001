{
  "hash": "74e7da1b570d6abe24fee0245677a5e0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Linear Regression\ndate: today\ndate-format: long\nfooter: \"[DATS 1001 Website](https://ds4all.rocks)\"\nlogo: images/ds4all-logo.png\nformat:\n  revealjs:\n    theme: [simple, custom.scss]\n    transition: fade\n    slide-number: true\n    #multiplex: true\n    chalkboard: true\nexecute:\n  echo: true\n  message: false\n  warning: false\n  freeze: auto\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n\n## Modeling \n\n::: {.incremental}\n- Use models to explore the relationship between variables and to make predictions\n- Explaining relationships (usually interested in causal relationships, but not always)\n    - Does oil wealth impact regime type?\n- Predictive modeling\n    - Where is violence most likely to happen in (country X) during their next election?\n    - Is this email spam?\n:::\n    \n## Modeling\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week-8.1_files/figure-html/linear-model-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n## Modeling\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week-8.1_files/figure-html/nonlinear-model-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n# Example: GDP per capita and Democracy\n\n## Pull in the VDEM Data\n\n<br>\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(vdemlite)\n\nmodel_data <- fetchdem(indicators = c(\"v2x_libdem\", \"e_gdppc\"),\n                       start_year = 2019, end_year = 2019) |>\n  rename(\n    country = country_name, \n    lib_dem = v2x_libdem, \n    wealth = e_gdppc\n    ) \n\nglimpse(model_data)\n```\n:::\n\n\n\n\n\n\n\n## Plot the Data\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week-8.1_files/figure-html/plot-wealth-dem-1-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n## Plot the Data\n\n<br>\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"0,1-3|4|5-10\"}\nggplot(model_data, aes(x = wealth, y = lib_dem)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"#E48957\", se = FALSE) +\n  scale_x_log10(label = scales::label_dollar(suffix = \"k\")) +\n  labs(\n    title = \"Wealth and Democracy, 2019\",\n    x = \"GPD per capita\", \n    y = \"Liberal Democracy Index\") +\n  theme_bw()\n```\n:::\n\n\n\n\n\n\n\n## Models as Functions\n\n::: {.incremental}\n- We can represent relationships between variables using **functions**\n- A function is a mathematical concept: the relationship between an output and one or more inputs\n  - Plug in the inputs and receive back the output\n- Example: The formula $y = 3x + 7$ is a function with input $x$ and output $y$. \n    - If $x$ is $5$, $y$ is $22$, \n    - $y = 3 \\times 5 + 7 = 22$\n:::\n\n## Quant Lingo {.smaller}\n\n<br>\n\n::: {.incremental}\n- **Response variable:** Variable whose behavior or variation you are trying to understand, on the y-axis in the plot\n    - **Dependent** variable\n    - **Outcome** variable\n    - **Y** variable\n- **Explanatory variables:** Other variables that you want to use to explain the variation in the response, on the x-axis in the plot\n    - **Independent** variables\n    - **Predictors**\n:::\n\n## \n\n<br>\n\nLinear model with one explanatory variable...\n\n::: {.incremental}\n- $Y = a + bX$\n- $Y$ is the outcome variable\n- $X$ is the explanatory variable\n- $a$ is the intercept: the predicted value of $Y$ when $X$ is equal to 0\n- $b$ is the slope of the line (rise over run)\n:::\n\n## Quant Lingo {.smaller}\n\n<br>\n\n::: {.incremental}\n- **Predicted value:** Output of the **model function**\n   - The model function gives the typical (expected) value of the response variable *conditioning* on the explanatory variables\n   - We often call this $\\hat{Y}$ to differentiate the predicted value from an observed value of Y in the data\n- **Residuals:** A measure of how far each case is from its predicted value (based on a particular model)\n  - Residual = Observed value ($Y$) - Predicted value ($\\hat{Y}$)\n  - How far above/below the expected value each case is\n:::\n\n##\n\n<br>\n\n::: {.callout-caution}\nNote that for the next few examples we will be analyzing GDP per capita on a log scale.\n:::\n\n## Residuals\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week-8.1_files/figure-html/residuals-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n## Linear Model\n\n$\\hat{Y} = a  + b \\times X$\n\n$\\hat{Y} = 0.13  + 0.12 \\times X$\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week-8.1_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n## Linear Model: Interpretation\n\n<br>\n\n| $\\hat{Y} = a  + b \\times X$\n| $\\hat{Y} = 0.13  + 0.12 \\times X$\n\nWhat is the interpretation of our estimate of $a$?\n\n. . .\n\n<br>\n\n| $\\hat{Y} = 0.13  + 0.12 \\times 0$\n| $\\hat{Y} = 0.13$\n\n$a$ is our predicted level of democracy when GDP per capita is 0.\n\n\n## Linear Model: Interpretation \n<br>\n\n\n| $\\hat{Y} = a  + b \\times X$\n| $\\hat{Y} = 0.13  + 0.12 \\times X$\n\nWhat is interpretation of our estimate of $b$?\n\n. . . \n\n<br>\n\n| $\\hat{Y} = a  + \\frac{Rise}{Run} \\times X$\n| $\\hat{Y} = a  + \\frac{Change Y}{Change X} \\times X$\n\n## Linear Model: Interpretation {.smaller}\n\n<br>\n\n| $b = \\frac{Change Y}{Change X}$\n| $0.12 = \\frac{Change Y}{Change X}$\n| ${Change Y} = 0.12 * {ChangeX}$\n\n. . .\n\n<br>\n\n| When $ChangeX = 1$:\n| ${Change Y = 0.12}$\n\n. . .\n\n<br>\n\n| $b$ is the predicted change in $Y$ **associated with** a ONE unit change in X.\n\n## Linear Model: Interpretation\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week-8.1_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n## Linear Model: Interpretation\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week-8.1_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n## Linear Model: Interpretation\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week-8.1_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n## Interpreting the Coefficient for log(Wealth)\n\n- **Model**: Democracy = 0.12 Ã— log(Wealth)\n  \n- **Coefficient Interpretation**: \n  - A 1% increase in GDP per capita is associated with a 0.0012 increase in the democracy score\n  - Doubling GDP per capita (e.g., from $10,000 to $20,000) increases the democracy score by 0.12 points\n\n- **In Dollar Terms**:\n  - If GDP per capita increases by 10% (e.g., from $10,000 to $11,000), the democracy score is expected to increase by 0.012 points\n\n## Linear Model: Interpretation\n\n<br>\n\nIs this the **causal** effect of GDP per capita on liberal democracy?\n\n. . .\n\n<br>\n\nNo! It is only the association...\n\n. . .\n\n<br>\n\nTo identify causality we need other methods (beyond the scope of this course).\n\n## Your Task {.smaller}\n\n<br> \n\nAn economist is interested in the relationship between years of education and hourly wages.  They estimate a linear model with estimates of $a$ and $b$ as follows:\n\n<br>\n\n$\\hat{Y} = 9 + 1.60*{YrsEdu}$\n\n<br>\n\n| 1. Interpret $a$ and $b$\n| 2. What is the predicted hourly wage for those with 10 years of education?\n| 3. How about for those with a high school diploma? (12 yrs)\n| 4. What about a college degree? (16 yrs)\n\n## Next step\n\n<br>\n\n- Linear model with one predictor: $Y = a + bX$\n- For any given data...\n- How do we figure out what the best values are for $a$ and $b$??\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n# Estimation\n\n## Linear Model with Single Predictor\n\n<br>\n\nGoal: Estimate Democracy score ($\\hat{Y_{i}}$) of a country given level of GDP per capita ($X_{i}$).\n\n<br>\n\nOr: Estimate relationship between GDP per capita and democracy.\n\n## Linear Model with Single Predictor\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week-8.1_files/figure-html/model-data-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n## Estimate Model \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel1 <-  lm(lib_dem ~ log_wealth, data = modelData) \n\nsummary(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = lib_dem ~ log_wealth, data = modelData)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.57441 -0.14334  0.03911  0.18730  0.37017 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.13051    0.03806   3.429 0.000758 ***\nlog_wealth   0.12040    0.01471   8.188 5.75e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2233 on 172 degrees of freedom\n  (5 observations deleted due to missingness)\nMultiple R-squared:  0.2805,\tAdjusted R-squared:  0.2763 \nF-statistic: 67.04 on 1 and 172 DF,  p-value: 5.754e-14\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n## \n\nIn equation form... How do we interpret the model?\n\n<br>\n\n$$\\widehat{Democracy}_{i} = 0.13 + 0.12 * {loggdppc}_{i}$$\n\n## Question\n\n<br>\n\nHow do we get the \"best\" values for the slope and intercept?\n\n\n## How would you draw the \"best\" line?\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week-8.1_files/figure-html/best-line-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n## How would you draw the \"best\" line?\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week-8.1_files/figure-html/best-line2-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n## Least squares regression\n\n<br>\n\n- Remember the residual is the difference between the actual value and the predicted value\n\n. . .\n\n- The regression line minimizes the sum of squared residuals.\n\n## Least squares regression\n\n<br>\n\n- Residual for each point is:  $e_i = y_i - \\hat{y}_i$\n\n- Least squares regression line minimizes $\\sum_{i = 1}^n e_i^2$.\n\n. . .\n\n- Why do we square the residual?\n\n. . .\n\n- Why not take absolute value?\n\n    - Principle: larger penalty for residuals further away\n    - Math: makes the math easier and some nice properties (not our concern here...)\n\n## Least squares regression\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week-8.1_files/figure-html/best-line3-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n## Very Simple Example\n\nWhat should the slope and intercept be?\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week-8.1_files/figure-html/best-line4-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n## Example\n\n$\\hat{Y} = 0 + 1*X$\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week-8.1_files/figure-html/best-line5-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n## Example\n\nWhat is the sum of squared residuals?\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week-8.1_files/figure-html/best-line6-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n## Example\n\nWhat is sum of squared residuals for $y = 0 + 0*X$?\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week-8.1_files/figure-html/best-line7-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n## Example\n\nWhat is sum of squared residuals for $y = 0 + 0*X$?\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week-8.1_files/figure-html/best-line8-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n(1-0)^2 + (2-0)^2 + (3-0)^2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 14\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n## Example\n\nWhat is sum of squared residuals for $y = 0 + 2*X$?\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week-8.1_files/figure-html/best-line10-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n## Example\n\nWhat is sum of squared residuals for $y = 0 + 2*X$?\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week-8.1_files/figure-html/best-line11-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n(1-2)^2 + (2-4)^2 + (3-6)^2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 14\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n## One more...\n\nWhat is sum of squared residuals for $y = 0 + -1*X$?\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week-8.1_files/figure-html/best-line13-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n## One more...\n\nWhat is sum of squared residuals for $y = 0 + -1*X$?\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week-8.1_files/figure-html/best-line14-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n(1+1)^2 + (2+2)^2 + (3+3)^2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 56\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n## Cost Function\n\nSum of Squared Residuals as function of possible values of $b$\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week-8.1_files/figure-html/cost-function-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n## Least Squares Regression\n\n<br>\n\n- When we estimate a least squares regression, it is looking for the line that minimizes sum of squared residuals\n\n- In the simple example, I set $a=0$ to make it easier.  More complicated when searching for combination of $a$ and $b$ that minimize, but same basic idea\n\n## Least Squares Regression\n\n<br>\n\n- There is a way to solve for this analytically for linear regression (i.e., by doing math...)\n\n    -- They made us do this in grad school...\n\n. . .\n\n- In machine learning, people also use gradient descent algorithm in which the computer searches over possible combinations of $a$ and $b$ until it settles on the lowest point.    \n\n## Least Squares Regression\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week-8.1_files/figure-html/ssr-viz-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n## Least Squares Regression\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week-8.1_files/figure-html/regression-line-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "week-8.1_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}